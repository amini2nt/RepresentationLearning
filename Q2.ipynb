{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"Convnet Classifier\"\"\"\n",
    "    def __init__(self, feature_maps, dropouts):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.feature_maps = feature_maps\n",
    "        self.dropouts = dropouts\n",
    "        layers = self.layer(1,feature_maps[0], dropouts[0])+self.layer(feature_maps[0],feature_maps[1], dropouts[1])+self.layer(feature_maps[1],feature_maps[2], dropouts[2])+self.layer(feature_maps[2],feature_maps[3], dropouts[3])+self.layer(feature_maps[3],feature_maps[4], dropouts[4])\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        self.clf = nn.Linear(feature_maps[-1], 10)\n",
    "        self.drop = nn.Dropout(p=0.4)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def layer(self, i, o, dropout=0.1):\n",
    "        return [nn.Conv2d(in_channels=i, out_channels=o, kernel_size=(4, 4), padding=2),\n",
    "                #nn.BatchNorm2d(o),\n",
    "                nn.Dropout(p=dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2), stride=2)]\n",
    "    \n",
    "    def num_parameters(self):\n",
    "        pars = 0\n",
    "        for a,b in zip([1] + self.feature_maps, self.feature_maps):\n",
    "            current_layer_parameters = a * 16 * b\n",
    "            pars = pars + current_layer_parameters\n",
    "        pars = pars + self.feature_maps[-1] * 10\n",
    "        print(\"we have \", float(pars)/1000000, \" million parameters\") # Ignore biases\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        y = x.squeeze()\n",
    "        return self.clf(self.drop(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n",
      "[20, 40, 60, 80, 128] [0, 0.1, 0.1, 0.1, 0.1]\n",
      "we have  0.29344  million parameters\n",
      "[(97.96, 0.3144947297569277), (98.86, 0.07668959631173532), (99.05, 0.05425320533332604), (99.19, 0.04697827352429313), (99.0, 0.03525888407702194), (99.1, 0.032066629496591684), (99.16, 0.027506706296905145), (99.09, 0.024175951048843007), (99.25, 0.022246938287171284), (99.16, 0.020036977282894063)]\n",
      "[20, 40, 60, 80, 128] [0, 0.4, 0.4, 0.1, 0.1]\n",
      "we have  0.29344  million parameters\n",
      "[(98.27, 0.329681209886252), (98.76, 0.09770918964569185), (98.97, 0.0694704822171281), (99.14, 0.060814434575087735), (99.11, 0.052461012558483365), (99.05, 0.047244481386930576), (99.31, 0.04391476701794148), (99.23, 0.03966553975095246), (99.19, 0.03760211830879293), (99.32, 0.0366828586560672)]\n",
      "[20, 40, 60, 80, 128] [0, 0.1, 0.5, 0.1, 0.5]\n",
      "we have  0.29344  million parameters\n",
      "[(98.56, 0.38310466095336526), (99.05, 0.08670556062121572), (98.89, 0.06535159540511588), (99.06, 0.05312962978204557), (99.27, 0.04465800848827243), (99.11, 0.0423341716911747), (99.24, 0.03636363019714001), (99.18, 0.034424508162247916), (99.11, 0.03148346264332311), (99.12, 0.03147428677177061)]\n",
      "[20, 40, 60, 50, 40] [0, 0.1, 0.1, 0.1, 0.1]\n",
      "we have  0.13192  million parameters\n",
      "[(97.56, 0.39413738150649996), (98.34, 0.11535558979441998), (98.69, 0.08513940995864903), (99.16, 0.06606675711855578), (99.17, 0.05713856719863186), (99.21, 0.04990115344746789), (99.09, 0.04625348738237802), (99.12, 0.038202260124451444), (99.24, 0.03802802367632323), (99.13, 0.03441973531177875)]\n",
      "[20, 40, 60, 50, 40] [0, 0.4, 0.4, 0.1, 0.1]\n",
      "we have  0.13192  million parameters\n"
     ]
    }
   ],
   "source": [
    "perf = {}\n",
    "\n",
    "torch.manual_seed(0)\n",
    "mnist_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, transform=mnist_transforms, download=True)\n",
    "mnist_test = torchvision.datasets.MNIST(root='./data', train=False, transform=mnist_transforms, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"Cuda available: %s\" % cuda_available)\n",
    "feature_maps = [[20,40,60,80,128], [20,40,60,50,40], [20,40,50,60,70]]\n",
    "dropouts = [[0, 0.1, 0.1, 0.1,0.1 ], [0, 0.4, 0.4, 0.1,0.1 ], [0, 0.1, 0.5, 0.1,0.5 ]]\n",
    "for feature_map in feature_maps:\n",
    "    for dropout in dropouts:\n",
    "        hyper = str(feature_map)+\" \"+str(dropout)\n",
    "        print(hyper)\n",
    "        perf[hyper] = []\n",
    "        clf = Classifier(feature_map, dropout)\n",
    "        clf.num_parameters()\n",
    "        if cuda_available:\n",
    "            clf = clf.cuda()\n",
    "        optimizer = torch.optim.Adam(clf.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss() #  LogSoftmax and NLLLoss\n",
    "\n",
    "        for epoch in range(10):\n",
    "            # Train\n",
    "            current_epoch_loss = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "                if cuda_available:\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(clf(inputs), targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                current_epoch_loss.append(loss.item())\n",
    "            myLoss =  np.mean(current_epoch_loss)\n",
    "            \n",
    "            #    if batch_idx%500==0:\n",
    "            #       print('Epoch : %d, Loss : %.3f ' % (epoch, np.mean(losses)))\n",
    "            \n",
    "            # Evaluate\n",
    "            clf.eval()\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "                if cuda_available:\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                outputs = clf(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets.data).cpu().sum()\n",
    "            accur = 100*float(correct)/total    \n",
    "            #print('Epoch : %d, Test Accuracy : %.2f%%, number of tests : %d' % (epoch, accur, total))\n",
    "            #print('--------------------------------------------------------------')\n",
    "            clf.train()\n",
    "            perf[hyper].append((accur, myLoss))\n",
    "        print(perf[hyper])\n",
    "\n",
    "\n",
    "        \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we select the  top combination of hyperparameters in the following:  \n",
    "and further plot the training error and validation accuracy for the ten epochs. The training error for each epoch indicates the mean of all the losses at each step during that epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_performance = max(perf.values(), key=lambda x: max([a[0] for a in x]))\n",
    "best_accuracy = [x[0] for x in best_performance]\n",
    "best_loss = [x[1] for x in best_performance]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotThis(x, label ):\n",
    "    xnew = np.linspace(np.arange(x.shape[0]).min(),np.arange(x.shape[0]).max(),300)\n",
    "    spl = make_interp_spline(np.arange(x.shape[0]),x, k=3)\n",
    "    power_smooth = spl(xnew)\n",
    "    xx = plt.plot(xnew,power_smooth, label = label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotThis(np.array(best_accuracy), \"accuracy\")\n",
    "\n",
    "plt.title(\"validation accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotThis(np.array(best_loss), \"error\")\n",
    "\n",
    "plt.title(\"training error\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have lowered the dropout from p=0.5 to p=0.1 or p=0.4 for various convolutional layers. This was inspired by Park et.al[1]. For the given problem, we find out that the above combination of dropout probabilites for different convolutional layers works the best for the purpose of reducing bias and making the model more generalizable. However, we know that presumably increasing the dropout probabilites may in "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repr learning",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
