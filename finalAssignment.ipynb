{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import wget\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotThis(x, label ):\n",
    "    xnew = np.linspace(np.arange(x.shape[0]).min(),np.arange(x.shape[0]).max(),300)\n",
    "    spl = make_interp_spline(np.arange(x.shape[0]),x, k=3)\n",
    "    power_smooth = spl(xnew)\n",
    "    xx = plt.plot(xnew,power_smooth, label = label)\n",
    "\n",
    "def vectorized_result(y):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[y] = 1.0\n",
    "    return e\n",
    "\n",
    "def load_mnist():\n",
    "    if not os.path.exists(os.path.join(os.curdir, 'data')):\n",
    "        os.mkdir(os.path.join(os.curdir, 'data'))\n",
    "        wget.download('http://deeplearning.net/data/mnist/mnist.pkl.gz', out='data')\n",
    "\n",
    "    data_file = gzip.open(os.path.join(os.curdir, 'data', 'mnist.pkl.gz'), 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(data_file, encoding='latin1')\n",
    "    data_file.close()\n",
    "\n",
    "    training_inputs = np.squeeze(np.asarray([np.reshape(x, (784, 1)) for x in training_data[0]]))\n",
    "    training_results = np.squeeze(np.asarray([vectorized_result(y) for y in training_data[1]]))\n",
    "\n",
    "    validation_inputs = np.squeeze(np.asarray([np.reshape(x, (784, 1)) for x in validation_data[0]]))\n",
    "    validation_results = np.asarray(validation_data[1])\n",
    "    \n",
    "    test_inputs = np.squeeze(np.asarray([np.reshape(x, (784, 1)) for x in test_data[0]]))\n",
    "    test_results = np.squeeze(np.asarray(test_data[1]))\n",
    "    return training_inputs, training_results, validation_inputs, validation_results, test_inputs, test_results\n",
    "\n",
    "training_inputs, training_results, validation_inputs, validation_results, test_inputs, test_results= load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "def softmaxx(z):\n",
    "        return softmax(z, axis=1)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return expit(z)\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return z*(1-z)\n",
    "\n",
    "def ReLU(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "def ReLU_prime(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - np.tanh(x)**2\n",
    "\n",
    "    \n",
    "class NN(object):\n",
    "    def __init__(self, network_units, activation_fn, lr, batch_size, initialization):\n",
    "        self.network_units = network_units\n",
    "        if activation_fn ==\"ReLU\":\n",
    "            self.activation_fn = ReLU\n",
    "            self.activation_p = ReLU_prime\n",
    "        elif activation_fn == \"sigmoid\":\n",
    "            self.activation_fn = sigmoid\n",
    "            self.activation_p = sigmoid_prime\n",
    "        elif activation_fn == \"tanh\":\n",
    "            self.activation_fn = tanh\n",
    "            self.activation_p = tanh_prime\n",
    "        self.parameters = {}\n",
    "        self.gradients = {}\n",
    "        self.initialization = initialization\n",
    "        self.initialize_weights()\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "    def initialize_weights(self):\n",
    "        \n",
    "        for i in range(len(self.network_units)-1):\n",
    "            \n",
    "            self.parameters[\"b\"+str(i)] = np.zeros((self.network_units[i+1]))\n",
    "            \n",
    "            if self.initialization == \"glorot\":\n",
    "                self.parameters[\"w\"+str(i)] = np.random.uniform( \n",
    "                                -np.sqrt(6./(self.network_units[i]+self.network_units[i+1])), \n",
    "                                np.sqrt(6./(self.network_units[i]+self.network_units[i+1])),\n",
    "                                size=(self.network_units[i],self.network_units[i+1])\n",
    "                                                              )\n",
    "            elif self.initialization == \"normal\":\n",
    "                self.parameters[\"w\"+str(i)] = np.random.normal(scale=1, size=(self.network_units[i],self.network_units[i+1]))\n",
    "            \n",
    "            elif self.initialization == \"zero\":\n",
    "                self.parameters[\"w\"+str(i)] = np.zeros(shape=(self.network_units[i],self.network_units[i+1]))\n",
    "    \n",
    "            self.gradients[\"b\"+str(i)] = np.zeros((self.network_units[i+1]))\n",
    "            self.gradients[\"w\"+str(i)] = np.zeros((self.network_units[i], self.network_units[i+1]))\n",
    "\n",
    "            \n",
    "    def num_parameters(self):\n",
    "        pars = 0.0\n",
    "        for i in range(len(self.network_units)-1):\n",
    "            pars += self.network_units[i]\n",
    "            pars += (self.network_units[i] * self.network_units[i+1])\n",
    "        print(\"we have \", pars/1000000, \" parameters\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.h1 = self.activation_fn(np.dot(x, self.parameters[\"w0\"]) + self.parameters[\"b0\"])\n",
    "        self.h2 = self.activation_fn(np.dot(self.h1, self.parameters[\"w1\"]) + self.parameters[\"b1\"])\n",
    "        self.out = expit(np.dot(self.h2, self.parameters[\"w2\"]) + self.parameters[\"b2\"])\n",
    "        return self.out\n",
    "    \n",
    "    def loss(self, y):\n",
    "        self.y = y\n",
    "        l = -y*np.log(self.out+1e-8)-(1-y)*np.log(1-self.out+1e-8)\n",
    "        m_b_l = np.sum(l, axis=1)\n",
    "        self.lossy = np.mean(m_b_l)\n",
    "        return self.lossy\n",
    "    \n",
    "    def backward(self):\n",
    "        \n",
    "        dout = (-self.y/(self.out+1e-8)) + (1-self.y) * (1/(1-self.out+1e-8))\n",
    "        del2 = self.out*(1-self.out)*dout\n",
    "        doutdw2 = np.dot(self.h2.T, del2)/len(self.x)\n",
    "        doutdb2 = np.mean(del2, axis=0)\n",
    "        \n",
    "        \n",
    "        douth2 = np.dot(del2, self.parameters[\"w2\"].T)\n",
    "        del1 = self.activation_p(self.h2)*douth2\n",
    "        doutdw1 = np.dot(self.h1.T, del1)/len(self.x)\n",
    "        doutdb1 = np.mean(del1, axis=0)\n",
    "        \n",
    "        douth1 = np.dot(del1, self.parameters[\"w1\"].T)\n",
    "        del0 = self.activation_p(self.h1)*douth1\n",
    "        doutdw0 = np.dot(self.x.T, del0)/5\n",
    "        doutdb0 = np.mean(del0, axis=0)\n",
    "        \n",
    "        self.gradients[\"w2\"] = doutdw2\n",
    "        self.gradients[\"b2\"] = doutdb2\n",
    "        self.gradients[\"w1\"] = doutdw1\n",
    "        self.gradients[\"b1\"] = doutdb1\n",
    "        self.gradients[\"w0\"] = doutdw0\n",
    "        self.gradients[\"b0\"] = doutdb0\n",
    "        \n",
    "    def update(self):\n",
    "        for ind in [\"w0\", \"w1\", \"w2\", \"b0\", \"b1\", \"b2\"]:\n",
    "            self.parameters[ind] -= self.lr * self.gradients[ind]\n",
    "\n",
    "    def train(self,training_inputs, training_results, epochs, validation_inputs, validation_targets):\n",
    "        loss = np.zeros((epochs))\n",
    "        accuracy = np.zeros((epochs))\n",
    "        for epoch in range(epochs):\n",
    "            shuffle(training_inputs, training_results, random_state=0)\n",
    "            mini_batches = [training_inputs[k:k+self.batch_size] for k in range(0, training_inputs.shape[0], self.batch_size)]\n",
    "            mini_batches_results = [training_results[k:k+self.batch_size] for k in range(0, training_results.shape[0], self.batch_size)]\n",
    "            #print(mini_batches_results[0])\n",
    "            #break\n",
    "            current_loss=0\n",
    "            for mini_batch_index in range(len(mini_batches)):\n",
    "                self.forward(mini_batches[mini_batch_index])\n",
    "                current_loss += self.loss(mini_batches_results[mini_batch_index])\n",
    "                self.backward()\n",
    "                self.update()\n",
    "            current_loss /= len(mini_batches)\n",
    "            loss[epoch] = current_loss\n",
    "            print(\"loss \", current_loss)\n",
    "            currentAccuray = self.test(validation_inputs, validation_targets)\n",
    "            accuracy[epoch] = currentAccuray\n",
    "        return loss, accuracy \n",
    "    def test(self, validation_inputs, validation_targets):\n",
    "        results = self.forward(validation_inputs)\n",
    "        labels = np.argmax(results, axis=1)\n",
    "        accuracy = np.sum(labels == validation_targets)/100\n",
    "        print(\"validation accuracy\", accuracy)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have  0.648784  parameters\n",
      "loss  0.8555615965852404\n",
      "validation accuracy 92.07\n",
      "loss  0.5520381454903184\n",
      "validation accuracy 93.49\n",
      "loss  0.4596304832885305\n",
      "validation accuracy 94.46\n",
      "loss  0.3954183745105703\n",
      "validation accuracy 95.14\n",
      "loss  0.3463237178635861\n",
      "validation accuracy 95.68\n",
      "loss  0.3070291366364814\n",
      "validation accuracy 96.08\n",
      "loss  0.2747972929814326\n",
      "validation accuracy 96.36\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-10097ffcb583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmyNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"glorot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmyNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mglorot_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglorot_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-221cdb44f6f9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_inputs, training_results, epochs, validation_inputs, validation_targets)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmini_batch_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batches_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmini_batch_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-221cdb44f6f9>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mdoutdb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mdouth1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"w1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mdel0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdouth1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mdoutdw0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "myNet = NN([784,500,500,10], \"tanh\", lr=0.01, batch_size=32, initialization=\"glorot\")\n",
    "myNet.num_parameters()\n",
    "glorot_loss, glorot_accuracy = myNet.train(training_inputs, training_results, 10, validation_inputs, validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have  0.648784  parameters\n",
      "loss  6.441519534814023\n",
      "validation accuracy 89.5\n",
      "loss  2.691303170255162\n",
      "validation accuracy 90.89\n",
      "loss  2.3399882993061283\n",
      "validation accuracy 91.93\n",
      "loss  2.074949211512992\n",
      "validation accuracy 92.76\n",
      "loss  1.8418252557862786\n",
      "validation accuracy 92.88\n",
      "loss  1.7208812577225567\n",
      "validation accuracy 92.87\n",
      "loss  1.5947421397379185\n",
      "validation accuracy 93.08\n",
      "loss  1.4978184609154608\n",
      "validation accuracy 93.81\n",
      "loss  1.4399244594829628\n",
      "validation accuracy 93.41\n",
      "loss  1.366474022877882\n",
      "validation accuracy 93.65\n"
     ]
    }
   ],
   "source": [
    "myNet = NN([784,500,500,10], \"tanh\", lr=0.01, batch_size=32, initialization=\"normal\")\n",
    "myNet.num_parameters()\n",
    "normal_loss, normal_accuracy = myNet.train(training_inputs, training_results, 10, validation_inputs, validation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNet = NN([784,500,500,10], \"ReLU\", lr=0.01, batch_size=32, initialization=\"zero\")\n",
    "myNet.num_parameters()\n",
    "zero_loss, zero_accuracy = myNet.train(training_inputs, training_results, 10, validation_inputs, validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotThis(loss)\n",
    "plt.title(\"evaluation loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
