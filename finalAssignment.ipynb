{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import wget\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotThis(x, label ):\n",
    "    xnew = np.linspace(np.arange(x.shape[0]).min(),np.arange(x.shape[0]).max(),300)\n",
    "    spl = make_interp_spline(np.arange(x.shape[0]),x, k=3)\n",
    "    power_smooth = spl(xnew)\n",
    "    xx = plt.plot(xnew,power_smooth, label = label)\n",
    "\n",
    "def vectorized_result(y):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[y] = 1.0\n",
    "    return e\n",
    "\n",
    "def load_mnist():\n",
    "    if not os.path.exists(os.path.join(os.curdir, 'data')):\n",
    "        os.mkdir(os.path.join(os.curdir, 'data'))\n",
    "        wget.download('http://deeplearning.net/data/mnist/mnist.pkl.gz', out='data')\n",
    "\n",
    "    data_file = gzip.open(os.path.join(os.curdir, 'data', 'mnist.pkl.gz'), 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(data_file, encoding='latin1')\n",
    "    data_file.close()\n",
    "\n",
    "    training_inputs = np.squeeze(np.asarray([np.reshape(x, (784, 1)) for x in training_data[0]]))\n",
    "    training_results = np.squeeze(np.asarray([vectorized_result(y) for y in training_data[1]]))\n",
    "\n",
    "    validation_inputs = np.squeeze(np.asarray([np.reshape(x, (784, 1)) for x in validation_data[0]]))\n",
    "    validation_results = np.asarray(validation_data[1])\n",
    "    \n",
    "    test_inputs = np.squeeze(np.asarray([np.reshape(x, (784, 1)) for x in test_data[0]]))\n",
    "    test_results = np.squeeze(np.asarray(test_data[1]))\n",
    "    return training_inputs, training_results, validation_inputs, validation_results, test_inputs, test_results\n",
    "\n",
    "training_inputs, training_results, validation_inputs, validation_results, test_inputs, test_results= load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "def softmaxx(z):\n",
    "        return softmax(z, axis=1)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return expit(z)\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return z*(1-z)\n",
    "\n",
    "def ReLU(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "def ReLU_prime(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n",
    "\n",
    "    \n",
    "class NN(object):\n",
    "    def __init__(self, network_units, activation_fn, lr, batch_size, initialization):\n",
    "        self.network_units = network_units\n",
    "        if activation_fn ==\"ReLU\":\n",
    "            self.activation_fn = ReLU\n",
    "            self.activation_p = ReLU_prime\n",
    "        elif activation_fn == \"sigmoid\":\n",
    "            self.activation_fn = sigmoid\n",
    "            self.activation_p = sigmoid_prime\n",
    "        elif activation_fn == \"tanh\":\n",
    "            self.activation_fn = tanh\n",
    "            self.activation_p = tanh_prime\n",
    "        self.parameters = {}\n",
    "        self.gradients = {}\n",
    "        self.initialization = initialization\n",
    "        self.initialize_weights()\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "    def initialize_weights(self):\n",
    "        \n",
    "        for i in range(len(self.network_units)-1):\n",
    "            \n",
    "            self.parameters[\"b\"+str(i)] = np.zeros((self.network_units[i+1]))\n",
    "            \n",
    "            if self.initialization == \"glorot\":\n",
    "                self.parameters[\"w\"+str(i)] = np.random.uniform( \n",
    "                                -np.sqrt(6./(self.network_units[i]+self.network_units[i+1])), \n",
    "                                np.sqrt(6./(self.network_units[i]+self.network_units[i+1])),\n",
    "                                size=(self.network_units[i],self.network_units[i+1])\n",
    "                                                              )\n",
    "            elif self.initialization == \"normal\":\n",
    "                self.parameters[\"w\"+str(i)] = np.random.normal(scale=1, size=(self.network_units[i],self.network_units[i+1]))\n",
    "            \n",
    "            elif self.initialization == \"zero\":\n",
    "                self.parameters[\"w\"+str(i)] = np.zeros(shape=(self.network_units[i],self.network_units[i+1]))\n",
    "    \n",
    "            self.gradients[\"b\"+str(i)] = np.zeros((self.network_units[i+1]))\n",
    "            self.gradients[\"w\"+str(i)] = np.zeros((self.network_units[i], self.network_units[i+1]))\n",
    "\n",
    "            \n",
    "    def num_parameters(self):\n",
    "        pars = 0.0\n",
    "        for i in range(len(self.network_units)-1):\n",
    "            pars += self.network_units[i]\n",
    "            pars += (self.network_units[i] * self.network_units[i+1])\n",
    "        print(\"we have \", pars/1000000, \" parameters\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.h1 = self.activation_fn(np.dot(x, self.parameters[\"w0\"]) + self.parameters[\"b0\"])\n",
    "        self.h2 = self.activation_fn(np.dot(self.h1, self.parameters[\"w1\"]) + self.parameters[\"b1\"])\n",
    "        self.out = expit(np.dot(self.h2, self.parameters[\"w2\"]) + self.parameters[\"b2\"])\n",
    "        return self.out\n",
    "    \n",
    "    def loss(self, y):\n",
    "        self.y = y\n",
    "        l = -y*np.log(self.out+1e-8)-(1-y)*np.log(1-self.out+1e-8)\n",
    "        m_b_l = np.sum(l, axis=1)\n",
    "        self.lossy = np.mean(m_b_l)\n",
    "        return self.lossy\n",
    "    \n",
    "    def backward(self):\n",
    "        \n",
    "        \n",
    "        dout = (-self.y/(self.out+1e-8)) + (1-self.y) * (1/(1-self.out+1e-8))\n",
    "        del2 = self.out*(1-self.out)*dout\n",
    "        doutdw2 = np.dot(self.h2.T, del2)/len(self.x)\n",
    "        doutdb2 = np.mean(del2, axis=0)\n",
    "        \n",
    "        \n",
    "        douth2 = np.dot(del2, self.parameters[\"w2\"].T)\n",
    "        del1 = self.activation_p(self.h2)*douth2\n",
    "        doutdw1 = np.dot(self.h1.T, del1)/len(self.x)\n",
    "        doutdb1 = np.mean(del1, axis=0)\n",
    "        \n",
    "        douth1 = np.dot(del1, self.parameters[\"w1\"].T)\n",
    "        del0 = self.activation_p(self.h1)*douth1\n",
    "        doutdw0 = np.dot(self.x.T, del0)/len(self.x)\n",
    "        doutdb0 = np.mean(del0, axis=0)\n",
    "        \n",
    "        self.gradients[\"w2\"] = doutdw2\n",
    "        self.gradients[\"b2\"] = doutdb2\n",
    "        self.gradients[\"w1\"] = doutdw1\n",
    "        self.gradients[\"b1\"] = doutdb1\n",
    "        self.gradients[\"w0\"] = doutdw0\n",
    "        self.gradients[\"b0\"] = doutdb0\n",
    "        \n",
    "    def grad_check(self, single_training_example, single_training_result, p, row):\n",
    "        output = self.forward(single_training_example)\n",
    "        loss = self.loss(single_training_result)\n",
    "        self.backward()\n",
    "        all_Ns = []\n",
    "        max_differences = []\n",
    "        for k in range(1,5):\n",
    "            for i in range(0,5):\n",
    "                N = k * math.pow(10,i)\n",
    "                all_Ns.append(N)\n",
    "                \n",
    "                epsilon = 1.0 / N\n",
    "\n",
    "                #finite_difference:\n",
    "                numerical_gradient = 0.0\n",
    "                max_difference = 0\n",
    "                for j in range(10):\n",
    "\n",
    "                    analytic_gradient = self.gradients[p][row][j]\n",
    "                    #left\n",
    "                    self.parameters[p][row][j] += epsilon\n",
    "                    self.forward(single_training_example)\n",
    "                    left_loss = self.loss(single_training_result)\n",
    "                    #right\n",
    "                    self.parameters[p][row][j] -= (2 * epsilon)\n",
    "                    self.forward(single_training_example)\n",
    "                    right_loss = self.loss(single_training_result)\n",
    "                    self.parameters[p][row][j] += epsilon\n",
    "                    print(\"\\n analytic gradient: \", analytic_gradient)\n",
    "                    numerical_gradient = (left_loss - right_loss)/ (2*epsilon)\n",
    "                    print(\"\\n numerical gradient: \", numerical_gradient)\n",
    "                    current_difference = np.abs(numerical_gradient - analytic_gradient)\n",
    "                    print(\"difference: \",current_difference)\n",
    "                    if current_difference > max_difference:\n",
    "                        max_difference = current_difference\n",
    "                max_differences.append(max_difference)\n",
    "        return all_Ns, max_differences\n",
    "        \n",
    "                \n",
    "\n",
    "    def update(self):\n",
    "        for ind in [\"w0\", \"w1\", \"w2\", \"b0\", \"b1\", \"b2\"]:\n",
    "            self.parameters[ind] -= self.lr * self.gradients[ind]\n",
    "            \n",
    "\n",
    "    def train(self,training_inputs, training_results, epochs, validation_inputs, validation_targets):\n",
    "        loss = np.zeros((epochs))\n",
    "        accuracy = np.zeros((epochs))\n",
    "        for epoch in range(epochs):\n",
    "            shuffle(training_inputs, training_results, random_state=0)\n",
    "            mini_batches = [training_inputs[k:k+self.batch_size] for k in range(0, training_inputs.shape[0], self.batch_size)]\n",
    "            mini_batches_results = [training_results[k:k+self.batch_size] for k in range(0, training_results.shape[0], self.batch_size)]\n",
    "            #print(mini_batches_results[0])\n",
    "            #break\n",
    "            current_loss=0\n",
    "            for mini_batch_index in range(len(mini_batches)):\n",
    "                self.forward(mini_batches[mini_batch_index])\n",
    "                current_loss += self.loss(mini_batches_results[mini_batch_index])\n",
    "                self.backward()\n",
    "                self.update()\n",
    "            current_loss /= len(mini_batches)\n",
    "            loss[epoch] = current_loss\n",
    "            print(\"loss \", current_loss)\n",
    "            currentAccuray = self.test(validation_inputs, validation_targets)\n",
    "            accuracy[epoch] = currentAccuray\n",
    "        return loss, accuracy \n",
    "    def test(self, validation_inputs, validation_targets):\n",
    "        results = self.forward(validation_inputs)\n",
    "        labels = np.argmax(results, axis=1)\n",
    "        accuracy = np.sum(labels == validation_targets)/100\n",
    "        print(\"validation accuracy\", accuracy)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 --> Initilization:\n",
    "For I ran the model with 32 as the mini batch size, 500 units for both hidden layers which makes the total number of parameters to be 0.648784 million. I used Tanh activation for the hidden layer non linearities. Learning rate was chosen to be 0.01.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss  1.1164179979846724\n",
      "validation accuracy 90.75\n",
      "loss  0.6950673664685939\n",
      "validation accuracy 91.95\n",
      "loss  0.6050231667101834\n",
      "validation accuracy 92.72\n",
      "loss  0.5399757484267349\n",
      "validation accuracy 93.49\n",
      "loss  0.4851334667886092\n",
      "validation accuracy 94.21\n",
      "loss  0.43879148612648244\n",
      "validation accuracy 94.78\n",
      "loss  0.4000609361711174\n",
      "validation accuracy 95.15\n",
      "loss  0.36755883622510893\n",
      "validation accuracy 95.48\n",
      "loss  0.3399095648145816\n",
      "validation accuracy 95.7\n",
      "loss  0.31603467490692777\n",
      "validation accuracy 95.93\n"
     ]
    }
   ],
   "source": [
    "myNet = NN([784,500, 500,10], \"tanh\", lr=0.01, batch_size=32, initialization=\"glorot\")\n",
    "glorot_tanh_loss, glorot_tanh_accuracy = myNet.train(training_inputs, training_results, 10, validation_inputs, validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNet.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNet = NN([784,500,500,10], \"tanh\", lr=0.01, batch_size=32, initialization=\"normal\")\n",
    "normal_tanh_loss, normal_tanh_accuracy = myNet.train(training_inputs, training_results, 10, validation_inputs, validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myNet = NN([784,500,500,10], \"tanh\", lr=0.01, batch_size=32, initialization=\"zero\")\n",
    "zero_tanh_loss, zero_tanh_accuracy = myNet.train(training_inputs, training_results, 10, validation_inputs, validation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.1:\n",
    "Above we have shown the loss for every epoch for the three types of initilization. \n",
    "\n",
    "# Part 1.2:\n",
    "The three plots for both losses and accuracies are plotted in the following:\n",
    "When we have a symmetric(in this case zero initilization) initilization method, we know that the gradients for each of those weights will also be symmetric so all the updates would be same which is in essence equivalent to having only one neuron instead of 500.\n",
    "\n",
    "When we have gaussian distribution then since we are breaking the symmetry we can learn better than symmetric ones like zero. Also since the standard deviation  is 1 which is large, it quickly saturates the gradients which will slow the learning. \n",
    "\n",
    "Using glorot gave the best accuracy and the main reason is possibly because we have neither the problem of gradient saturation nor symmetry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plotThis(glorot_tanh_loss, \"glorot tanh loss\")\n",
    "plotThis(zero_tanh_loss, \"zero tanh loss\")\n",
    "plotThis(normal_tanh_loss, \"normal tanh loss\")\n",
    "\n",
    "plt.title(\"training loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plotThis(glorot_tanh_accuracy, \"glorot tanh accuracy\")\n",
    "plotThis(zero_tanh_accuracy, \"zero tanh accuracy\")\n",
    "plotThis(normal_tanh_accuracy, \"normal tanh accuracy\")\n",
    "\n",
    "plt.title(\"evaluation accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 \n",
    "For the hyperparameter search, we test with the 450, 500, 550, 600, 650 as the architecture model. For the case of NonLinearity we will test with ReLU, sigmoid and tanh and for the learning rate we will try 0.001, 0.01, 0.1. I will test mini batch size for 32, 64, and 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = [450, 500, 550, 600, 650]\n",
    "non_linearity = [\"ReLU\", \"sigmoid\", \"tanh\"]\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "mini_batch_size = [32, 64, 128]\n",
    "best_accuracy = 0\n",
    "best_arch = None \n",
    "best_non_linearity = None\n",
    "best_learning_rate = None\n",
    "best_mini_batch_size = None\n",
    "for i in range(len(architecture)):\n",
    "    for j in range(len(non_linearity)):\n",
    "        for k in range(len(learning_rate)):\n",
    "            for l in range(len(mini_batch_size)):\n",
    "                myNet = NN([784,architecture[i], architecture[i], 10], non_linearity[j], lr=learning_rate[k], batch_size=mini_batch_size[l], initialization=\"glorot\")\n",
    "                -, accuracy = myNet.train(training_inputs, training_results, 10, validation_inputs, validation_results)\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_arch = i \n",
    "                    best_non_linearity = j\n",
    "                    best_learning_rate = k\n",
    "                    best_mini_batch_size = l\n",
    "print(\"we achieved best accuracy of \", best_accuracy, \"with the following hyperparameters:\\n \",\n",
    "                          architecture[best_arch], \" --> units for both hidden layers\\n\",\n",
    "                          non_linearity[best_non_linearity], \"--> the non Linearity chosen for the hidden layer activations\\n\",\n",
    "                          learning_rate[best_learning_rate], \"--> the learning rate\\n\", \n",
    "                          mini_batch_size[best_mini_batch_size], \"--> the size of each mini batch\\n\"\n",
    "                        \n",
    "                          \n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "In the following we perform gradient checking for different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNet.grad_check(np.asarray([training_inputs[0]]), training_results[0], \"w0\", 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above all the gradients are zero for the first ten elements(the very first 10 elements of the 1st row) so the difference does not have a real meaning. It will be zero because corner of the image is all black which means all zero in the corner so the weights are going to end up az zero too. However, in the following you can see the gradient not being zero for the 161st row because the input corresponding to this weight is not zero. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNet.grad_check(np.asarray([training_inputs[0]]), training_results[0], \"w0\", 161)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.001853117398069204\n",
      "difference:  5.58275591268061e-06\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.003067361304479599\n",
      "difference:  2.0529148406508903e-05\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011154112934348981\n",
      "difference:  9.531102499541616e-05\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.002735754633037546\n",
      "difference:  2.844397875967396e-05\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.00041933110705033583\n",
      "difference:  6.7455111443878875e-06\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.004190538819989209\n",
      "difference:  1.2073374190414363e-05\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.00368982586330574\n",
      "difference:  1.879992840491144e-05\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007241585773087866\n",
      "difference:  3.2037504370847772e-06\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.0037133684404811462\n",
      "difference:  2.401368325629944e-05\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.0035034703180292226\n",
      "difference:  2.6575939282891085e-05\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.001847591855191344\n",
      "difference:  5.7213034820608605e-08\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.0030876838568794085\n",
      "difference:  2.0659600669927988e-07\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011248461193703041\n",
      "difference:  9.62765641356056e-07\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.0027075961514566593\n",
      "difference:  2.854971787873313e-07\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.0004126527833181459\n",
      "difference:  6.718741219798171e-08\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.0041785893167778365\n",
      "difference:  1.238709790418041e-07\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.003708437408337373\n",
      "difference:  1.883833732787936e-07\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007273303016097632\n",
      "difference:  3.2026136108213345e-08\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.003737140580738485\n",
      "difference:  2.415429989606824e-07\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.00352977829593093\n",
      "difference:  2.6796138118383855e-07\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.001847535214427598\n",
      "difference:  5.722710747120552e-10\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.0030878883867879736\n",
      "difference:  2.0660981341590057e-09\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011249414330699592\n",
      "difference:  9.628644805081965e-09\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.0027073135093758083\n",
      "difference:  2.855097936283607e-09\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.00041258626775064755\n",
      "difference:  6.718446996108705e-10\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.0041784666848232455\n",
      "difference:  1.2390244507470838e-09\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.003708623907855424\n",
      "difference:  1.883855227629433e-09\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007273620074577103\n",
      "difference:  3.202881611189326e-10\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.003737379708190769\n",
      "difference:  2.415546676744207e-09\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.0035300435774765493\n",
      "difference:  2.6798355643939398e-09\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.001847534647658744\n",
      "difference:  5.5022206409127694e-12\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.0030878904321296474\n",
      "difference:  2.075646035257228e-11\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011249423862991215\n",
      "difference:  9.63531823772179e-11\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.0027073106830255433\n",
      "difference:  2.8747671344114822e-11\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.0004125856025716246\n",
      "difference:  6.665676636954249e-12\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.004178465458248848\n",
      "difference:  1.2450053141210837e-11\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.0037086257725915672\n",
      "difference:  1.9119084353896865e-11\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007273623245929173\n",
      "difference:  3.152954134756636e-12\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.0037373820996888796\n",
      "difference:  2.4048566089895917e-11\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.0035300462305487557\n",
      "difference:  2.6763358022818817e-11\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.0018475346430513184\n",
      "difference:  8.947950887183698e-13\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.0030878904538900187\n",
      "difference:  1.003910930080787e-12\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011249423959025506\n",
      "difference:  3.188907471418645e-13\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.0027073106534380997\n",
      "difference:  8.397722621455994e-13\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.00041258559291268426\n",
      "difference:  2.9932636772846133e-12\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.004178465446980084\n",
      "difference:  1.1812894412654984e-12\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.0037086257903551356\n",
      "difference:  1.35551595989436e-12\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.000727362328145631\n",
      "difference:  3.9975954404386504e-13\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.0037373821232256077\n",
      "difference:  5.118379678425988e-13\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.003530046258637398\n",
      "difference:  1.3252845001976432e-12\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.001848956529494017\n",
      "difference:  1.421887337493561e-06\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.0030827334776755144\n",
      "difference:  5.156975210593353e-06\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011225413787312788\n",
      "difference:  2.40101720316091e-05\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.002714441714888105\n",
      "difference:  7.131060610233043e-06\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.00041426690368595587\n",
      "difference:  1.6813077800079275e-06\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.00418154310906349\n",
      "difference:  3.0776632646955376e-06\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.0037039184852407647\n",
      "difference:  4.7073064698868565e-06\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007265615913295553\n",
      "difference:  8.00736416316076e-07\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.003731352082775863\n",
      "difference:  6.030040961582835e-06\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.0035233606418776375\n",
      "difference:  6.685615434476148e-06\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.0018475489480540563\n",
      "difference:  1.430589753300196e-08\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.0030878388014043967\n",
      "difference:  5.165148171107012e-08\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011249183249457673\n",
      "difference:  2.407098867243662e-07\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.002707382030562311\n",
      "difference:  7.137628443886282e-08\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.00041260239225127116\n",
      "difference:  1.6796345323215247e-08\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.004178496419522393\n",
      "difference:  3.097372359810169e-08\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.003708578695161835\n",
      "difference:  4.709654881666567e-08\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007273543212371614\n",
      "difference:  8.006508710038142e-09\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.0037373217353242616\n",
      "difference:  6.038841318407179e-08\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.003529979262766325\n",
      "difference:  6.699454578856096e-08\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.0018475347852375812\n",
      "difference:  1.4308105785246217e-10\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.0030878899363595558\n",
      "difference:  5.165265519989359e-10\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011249421552150807\n",
      "difference:  2.4071935902925112e-09\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.002707311367999843\n",
      "difference:  7.137219708470977e-10\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.0004125857638648256\n",
      "difference:  1.67958877654497e-10\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.004178465755511063\n",
      "difference:  3.097122679845965e-10\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.003708625320775205\n",
      "difference:  4.709354464553506e-10\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007273622476988706\n",
      "difference:  8.004700082029498e-11\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.003737381519830496\n",
      "difference:  6.039069496213689e-10\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.003530045587363251\n",
      "difference:  6.699488628789108e-10\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.0018475346439394968\n",
      "difference:  1.782973508418495e-12\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.0030878904478948144\n",
      "difference:  4.9912934028950584e-12\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011249423934933667\n",
      "difference:  2.441073038150776e-11\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.0027073106610986386\n",
      "difference:  6.820766607767981e-12\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.00041258559757562097\n",
      "difference:  1.6696730261410442e-12\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.004178465448867463\n",
      "difference:  3.0686685831282645e-12\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.0037086257874685558\n",
      "difference:  4.242095823919767e-12\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007273623268133633\n",
      "difference:  9.325080855063228e-13\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.003737382117785515\n",
      "difference:  5.951930788505866e-12\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.003530046250865837\n",
      "difference:  6.4462766721784526e-12\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.0018475346430513184\n",
      "difference:  8.947950887183698e-13\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.003087890451114461\n",
      "difference:  1.7716466314821044e-12\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011249423959025506\n",
      "difference:  3.188907471418645e-13\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.002707310656768769\n",
      "difference:  2.4908968117298702e-12\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.00041258559679846485\n",
      "difference:  8.925169089034346e-13\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.004178465452531199\n",
      "difference:  6.732404564391281e-12\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.003708625788689801\n",
      "difference:  3.020850496832095e-12\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.000727362329255854\n",
      "difference:  1.5099825686690216e-12\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.0037373821260011653\n",
      "difference:  2.2637195937202925e-12\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.003530046258637398\n",
      "difference:  1.3252845001976432e-12\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.0018481687610458208\n",
      "difference:  6.341188892975032e-07\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.0030855964262307967\n",
      "difference:  2.294026655311112e-06\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011238737620650308\n",
      "difference:  1.068633869408947e-05\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.0027104816540126087\n",
      "difference:  3.1709997347366577e-06\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.0004133324264996019\n",
      "difference:  7.468305936539506e-07\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.004179838208182796\n",
      "difference:  1.3727623840009001e-06\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.0037065330678596187\n",
      "difference:  2.092723851032912e-06\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007270064657556308\n",
      "difference:  3.5586199024063934e-07\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.003734699910967365\n",
      "difference:  2.682212770080579e-06\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.003527071423825079\n",
      "difference:  2.9748334870348045e-06\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.0018475410005580617\n",
      "difference:  6.358401538359565e-09\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.0030878674964701647\n",
      "difference:  2.2956415943100467e-08\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011249316975658341\n",
      "difference:  1.0698368605593922e-07\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.0027073423772405736\n",
      "difference:  3.1722962701586505e-08\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.00041259306090335457\n",
      "difference:  7.464997406629505e-09\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.004178479212385566\n",
      "difference:  1.376658677158804e-08\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.003708604859845588\n",
      "difference:  2.0931865063727323e-08\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007273587693046313\n",
      "difference:  3.5584412401293633e-09\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.0037373552842168145\n",
      "difference:  2.6839520631156966e-08\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.003530016481609888\n",
      "difference:  2.9775702225891265e-08\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.0018475347058399814\n",
      "difference:  6.368345802955366e-11\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.003087890223218981\n",
      "difference:  2.29667126896288e-10\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011249422889431093\n",
      "difference:  1.0699133037861408e-09\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.0027073109714725874\n",
      "difference:  3.1719471537197674e-10\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.0004125856705949893\n",
      "difference:  7.468904135573759e-11\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.004178465583487556\n",
      "difference:  1.3768876143405162e-10\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.0037086255823493004\n",
      "difference:  2.0936135117222143e-10\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007273622922132628\n",
      "difference:  3.5532608647949326e-11\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.0037373818552954847\n",
      "difference:  2.684419609343125e-10\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.003530045959493355\n",
      "difference:  2.9781875880360853e-10\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.0018475346424406958\n",
      "difference:  2.8417242517453367e-13\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.003087890450004238\n",
      "difference:  2.881869656107261e-12\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011249423948034298\n",
      "difference:  1.1310098690930914e-11\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.00270731065682428\n",
      "difference:  2.546407962961128e-12\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.000412585596853976\n",
      "difference:  9.480280601346924e-13\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.004178465447646218\n",
      "difference:  1.8474232560405923e-12\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.003708625789800024\n",
      "difference:  1.9106274722069383e-12\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007273623273684748\n",
      "difference:  3.7739657319374453e-13\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.0037373821209496505\n",
      "difference:  2.7877951683241697e-12\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.0035300462551401957\n",
      "difference:  2.1719180273716e-12\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.0018475346535984372\n",
      "difference:  1.1441913822657357e-11\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.0030878904550002417\n",
      "difference:  2.1141339547059435e-12\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011249423959025506\n",
      "difference:  3.188907471418645e-13\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.0027073106573238803\n",
      "difference:  3.0460083240424485e-12\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.0004125855890269037\n",
      "difference:  6.879044263472661e-12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.004178465446980084\n",
      "difference:  1.1812894412654984e-12\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.003708625793130693\n",
      "difference:  1.4200416016685313e-12\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007273623253700734\n",
      "difference:  2.3757980175190263e-12\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.0037373821221153847\n",
      "difference:  1.6220609924677554e-12\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.0035300462569720636\n",
      "difference:  3.400500367400916e-13\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.0018478917615731838\n",
      "difference:  3.5711941666046257e-07\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.0030865996614899416\n",
      "difference:  1.290791396166193e-06\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011243409905186352\n",
      "difference:  6.014054158044735e-06\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.0027090946638941293\n",
      "difference:  1.7840096162573434e-06\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.0004130056059579168\n",
      "difference:  4.200100519688595e-07\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.004179238592933565\n",
      "difference:  7.731471347704327e-07\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.0037074485194945606\n",
      "difference:  1.1772722160910236e-06\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007271621596651734\n",
      "difference:  2.0016808069801915e-07\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.0037358729468544016\n",
      "difference:  1.5091768830440347e-06\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.0035283722335466283\n",
      "difference:  1.6740237654853593e-06\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.0018475382187932432\n",
      "difference:  3.5766367198659943e-09\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.003087877539873496\n",
      "difference:  1.2913012611883573e-08\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011249363780720945\n",
      "difference:  6.017862345247815e-08\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.0027073284984791\n",
      "difference:  1.784420122817945e-08\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.0004125897949625035\n",
      "difference:  4.199056555535732e-09\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.004178473189606624\n",
      "difference:  7.74380782934958e-09\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.003708614017527978\n",
      "difference:  1.1774182673545547e-08\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007273603261159778\n",
      "difference:  2.001629893625713e-09\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.0037373670264706504\n",
      "difference:  1.509726679522344e-08\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.0035300295084139677\n",
      "difference:  1.674889814600594e-08\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.0018475346778679125\n",
      "difference:  3.571138914096328e-11\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.0030878903237718802\n",
      "difference:  1.2911422755598756e-10\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011249423357617694\n",
      "difference:  6.017267031865892e-10\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.002707310832739118\n",
      "difference:  1.7846124621481718e-10\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.0004125856377878989\n",
      "difference:  4.1881950978064214e-11\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.004178465523252406\n",
      "difference:  7.745361123301375e-11\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.0037086256739593537\n",
      "difference:  1.1775129786159577e-10\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007273623077619362\n",
      "difference:  1.998393518807401e-11\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.0037373819727237745\n",
      "difference:  1.5101367118602882e-10\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.0035300460898834984\n",
      "difference:  1.6742861524282615e-10\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.0018475346417190508\n",
      "difference:  4.374725408318181e-13\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.0030878904513365057\n",
      "difference:  1.5496020265570731e-12\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011249423953252347\n",
      "difference:  6.0920504751926785e-12\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.0027073106558805904\n",
      "difference:  1.602718392029745e-12\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.00041258559613233103\n",
      "difference:  2.2638309412834068e-13\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.004178465447202129\n",
      "difference:  1.4033340461905297e-12\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.0037086257906882025\n",
      "difference:  1.022449052506813e-12\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007273623270354079\n",
      "difference:  7.104634805812915e-13\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.003737382121560273\n",
      "difference:  2.1771725047803336e-12\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.0035300462561949075\n",
      "difference:  1.1172061539777012e-12\n",
      "\n",
      " analytic gradient:  -0.0018475346421565233\n",
      "\n",
      " numerical gradient:  -0.0018475346430513184\n",
      "difference:  8.947950887183698e-13\n",
      "\n",
      " analytic gradient:  -0.0030878904528861078\n",
      "\n",
      " numerical gradient:  -0.0030878904611064684\n",
      "difference:  8.220360590144304e-12\n",
      "\n",
      " analytic gradient:  -0.011249423959344397\n",
      "\n",
      " numerical gradient:  -0.011249423963466398\n",
      "difference:  4.122001351358762e-12\n",
      "\n",
      " analytic gradient:  -0.002707310654277872\n",
      "\n",
      " numerical gradient:  -0.0027073106534380997\n",
      "difference:  8.397722621455994e-13\n",
      "\n",
      " analytic gradient:  0.00041258559590594794\n",
      "\n",
      " numerical gradient:  0.00041258559679846485\n",
      "difference:  8.925169089034346e-13\n",
      "\n",
      " analytic gradient:  0.004178465445798795\n",
      "\n",
      " numerical gradient:  0.004178465453641422\n",
      "difference:  7.842627589016438e-12\n",
      "\n",
      " analytic gradient:  0.0037086257917106516\n",
      "\n",
      " numerical gradient:  0.0037086258042329234\n",
      "difference:  1.2522271847920097e-11\n",
      "\n",
      " analytic gradient:  0.0007273623277458714\n",
      "\n",
      " numerical gradient:  0.0007273623348069691\n",
      "difference:  7.061097691794804e-12\n",
      "\n",
      " analytic gradient:  -0.0037373821237374457\n",
      "\n",
      " numerical gradient:  -0.003737382119339827\n",
      "difference:  4.397618554030647e-12\n",
      "\n",
      " analytic gradient:  -0.0035300462573121137\n",
      "\n",
      " numerical gradient:  -0.003530046255306729\n",
      "difference:  2.0053845736778264e-12\n"
     ]
    }
   ],
   "source": [
    "N, max_differences = myNet.grad_check(np.asarray([training_inputs[0]]), training_results[0], \"w1\", 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above I tried for the second layer weights and you can properly see that the gradient checking is getting done appropriately. Since the second layers gives me more accurate gradient difference between the numerical and analytic gradient I will plot that only against N:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAEKCAYAAAAiizNaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XucXVV99/HPl0kCAYFAiNYkQIKGaGhTAlMuxSo3m4Book1rUPrwIJVWQUBqKik+lWKpYl4StAUUAbmJ4SLCiNKgEkVRLhMDCQQjI6BkoBJDEhFjSOLv+WOvIScn57LPMPvMzJnv+/U6r9l77b3X/p09SX7Za6+9liICMzOzZtmhvwMwM7OhxYnHzMyayonHzMyayonHzMyayonHzMyayonHzMyaqtDEI2mGpJWSuiSdW2H7jpJuStsfkDShZNu8VL5S0vR6dUo6I5WFpL1KyiXpC2nbMkkHFfeNzcysnsISj6Q24FLgOGAKcKKkKWW7nQqsjYg3AguAi9KxU4A5wAHADOAySW116rwPOBb4Zdk5jgMmpc9pwOV9+T3NzKwxRd7xHAJ0RcSTEfEysBCYWbbPTODatHwrcIwkpfKFEbExIp4CulJ9VeuMiKUR8XSFOGYC10XmfmCUpNf36Tc1M7PchhVY9zjgmZL1VcCh1faJiM2S1gOjU/n9ZceOS8v16swTxzjguWoH7LXXXjFhwoQ61ZqZWaklS5b8JiLG1NuvyMQzqEg6jawpjn322YfOzs5+jsjMbHCRVP6oo6Iim9q6gb1L1sensor7SBoG7A6sqXFsnjp7EwcRcUVEtEdE+5gxdRO2mZn1UpGJ5yFgkqSJkkaQdRboKNunAzg5Lc8G7ols1NIOYE7q9TaRrGPAgznrLNcB/J/Uu+0wYH1EVG1mMzOzYhXW1Jae2ZwBLALagKsj4jFJFwCdEdEBXAVcL6kLeIEskZD2uxlYAWwGTo+ILZB1my6vM5WfCfwL8CfAMknfjoh/AL4NHE/WQeH3wClFfWczM6tPnhZhe+3t7eFnPGZmjZG0JCLa6+3nkQvMzKyp3Kutj9y+tJv5i1by7LoNjB01krnTJzNr2rj6B5qZDTFOPH3g9qXdzLttORs2bQGge90G5t22HMDJx8ysjJva+sD8RStfSTo9NmzawvxFK/spIjOzgcuJpw88u25DQ+VmZkOZE08fGDtqZEPlZmZDmRNPH5g7fTIjh7dtUzZyeBtzp0/up4jMzAYudy7oAz0dCNyrzcysPieePjJr2jgnGjOzHNzUZmZmTeXEY2ZmTeXEY2ZmTeXEY2ZmTeXEY2ZmTeXEY2ZmTdVQ4pG0g6TdigrGzMxaX93EI+lGSbtJ2gV4FFghaW7xoZmZWSvKc8czJSJ+C8wC7gImAn9faFRmZtay8iSe4ZKGkyWejojYVHBMZmbWwvIkni8BTwO7APdK2hdYX2RQZmbWuvIknm9GxLiIOD4iAvgV8IGC4zIzsxaVJ/F8vXQlJZ+FxYRjZmatruro1JLeBBwA7C7pPSWbdgN2KjowMzNrTbWmRZgMnACMAt5ZUv4i8MEigzIzs9ZVNfFExB3AHZIOj4ifNDEmMzNrYXkmguuS9K/AhNL9I8IdDMzMrGF5Es8dwA+B7wJbig3HzMxaXZ7Es3NEfLzwSMzMbEjI0536TknHFx6JmZkNCXkSz1lkyecPkn4r6UVJvy06MDMza011m9oiYtdmBGJmZkNDnmkRJOkkSf8vre8t6ZDiQzMzs1aUp6ntMuBw4H1p/XfApYVFZGZmLS1P4jk0Ik4H/gAQEWuBEXkqlzRD0kpJXZLOrbB9R0k3pe0PSJpQsm1eKl8paXq9OiVNTHV0pTpHpPJ9JC2WtFTSMneUMDPrX3kSzyZJbUAASBoD/LHeQemYS4HjgCnAiZKmlO12KrA2It4ILAAuSsdOAeaQjRU3A7hMUludOi8CFqS61qa6AT4B3BwR01Kdl+X4zmZmVpA8iecLwDeA10q6EPgR8J85jjsE6IqIJyPiZbIRrWeW7TMTuDYt3wocI0mpfGFEbIyIp4CuVF/FOtMxR6c6SHXOSstBNrApwO7AszliNzOzguTp1fZVSUuAYwABsyLi8Rx1jwOeKVlfBRxabZ+I2CxpPTA6ld9fduy4tFypztHAuojYXGH/84G7JX2EbDK7YysFK+k04DSAffbZJ8fXMzOz3qh6xyNpt/RzT+B54GvAjcCvU9lgcSJwTUSMB44Hrpe03feOiCsioj0i2seMGdP0IM3Mhopadzw3kk2LsISsuUplP/erU3c3sHfJ+vhUVmmfVZKGkTWFralzbKXyNcAoScPSXU/p/qeSPSciIn4iaSdgL7JkamZmTVb1jiciTkg/J0bEfuU/c9T9EDAp9TYbQfZgv6Nsnw7g5LQ8G7gnzXDaAcxJvd4mApOAB6vVmY5ZnOog1XlHWv4VWTMhkt5MNond6hzxm5lZAWrNQHpQrQMj4qd1tm+WdAawCGgDro6IxyRdAHRGRAdwFVnTVxfwAlkiIe13M7AC2AycHhFbUlzb1ZlO+XFgoaT/AJamugH+GfiypI+S3an935SozMysH6jav8GSFqfFnYB24BGyZrapZInj8KZE2A/a29ujs7Ozv8MwMxtUJC2JiPZ6+9VqajsqIo4CngMOSg/eDwamsf2zGjMzs1zyvMczOSKW96xExKPAm4sLyczMWlmeieCWSboSuCGtvx9YVlxIZmbWyvIknlOAD5HNywNwL3B5YRGZmVlLyzNywR/IxlFbUHw4ZmbW6uomHkmTgE+TDcq5U095znd5zMzMtpGnc8FXyJrWNgNHAdex9XmPmZlZQ/IknpER8T2yd35+GRHnA+8oNiwzM2tVeToXbEyDaj6RRg3oBl5TbFhmZtaq8tzxnAXsDJwJHAycxNbx1czMzBpS844nzfj53oj4GPA7sq7VZmZmvVbzjicNzPmWJsViZmZDQJ5nPEsldQC3AC/1FEbEbYVFZWZmLStP4tmJbKK1o0vKAnDiMTOzhuUZucDPdczMrM/kGbngCxWK15PNyXNHhW1mZmZV5elOvRNwIPBE+kwFxgOnSrqkwNjMzKwF5XnGMxU4omTq6cuBH5L1dlte60AzM7Nyee549mDbkQp2AfZMiWhjIVGZmVnLynPH81ngYUnfBwS8FfhPSbsA3y0wNjMza0F5erVdJenbwCGp6F8j4tm0PLewyMzMrCXlueMhIp4D3IPNzMxetTzPeMzMzPqME4+ZmTVVrqY2SX8O/FVa/WFEPFJcSGZm1srq3vFIOgv4KvDa9LlB0keKDszMzFpTnjueU4FDI+IlAEkXAT8B/qvIwMzMrDXlecYjYEvJ+pZUZmZm1rA8dzxfAR6Q9I20Pgu4qriQzMysleV5gfTiNGpBz0ykp0TE0kKjMjOzlpVnWoRPAfcCV/U85zEzM+utPM94ngROBDolPSjpc5JmFhyXmZm1qLqJJyK+EhEfAI4CbgD+Nv2sS9IMSSsldUk6t8L2HSXdlLY/IGlCybZ5qXylpOn16pQ0MdXRleocUbLt7yStkPSYpBvzxG5mZsXI8x7PlZJ+DFxO1jQ3m2yqhHrHtQGXAscBU4ATJU0p2+1UYG1EvBFYAFyUjp0CzAEOAGYAl0lqq1PnRcCCVNfaVDeSJgHzyOYUOgA4u17sZmZWnDxNbaOBNmAd8ALwm4jYnOO4Q4CuiHgyIl4GFgLlTXQzgWvT8q3AMZKUyhdGxMaIeAroSvVVrDMdc3Sqg1TnrLT8QeDSiFgLEBHP54jdzMwKkqep7d0RcSjZvDyjgMWSVuWoexzwTMn6qlRWcZ+UzNaTJbpqx1YrHw2sK0mIpefaH9hf0n2S7pc0o1Kwkk6T1Cmpc/Xq1Tm+npmZ9UaeXm0nkI3T9layxHMP2dTXg8UwYBJwJDAeuFfSn0XEutKdIuIK4AqA9vb2aHaQZmZDRZ4XSGeQJZrPl0wAl0c3sHfJ+vhUVmmfVZKGAbsDa+ocW6l8DTBK0rB011O6/yrggYjYBDwl6edkieihBr6LmZn1kTzPeF6KiJtKk04ar62eh4BJqbfZCLLOAh1l+3QAJ6fl2cA9ERGpfE7q9TaRLFE8WK3OdMziVAepzp6J624nu9tB0l5kTW9P5ojfzMwKkCfxvL1C2XH1Dkp3HmcAi4DHgZsj4jFJF0h6V9rtKmC0pC7gHODcdOxjwM3ACuB/gNMjYku1OlNdHwfOSXWNZuuwPouANZJWkCWnuRGxJsf3NjOzAii7WaiwQfoQ8GFgP+AXJZt2Be6LiJOKD69/tLe3R2dnZ3+HYWY2qEhaEhHt9far9YznRuAu4NOkO5HkxYh44VXGZ2ZmQ1TVxBMR68m6N5/YvHDMzKzV5XnGY2Zm1meceMzMrKlyJR5J+0o6Ni2PlLRrsWGZmVmryjNI6AfJxkD7UioaT/ZujJmZWcPy3PGcDhwB/BYgIp4AXltkUGZm1rryJJ6NaSRoANLQNh7LzMzMeiVP4vmBpH8FRkp6O3AL8M1iwzIzs1aVJ/GcC6wGlgP/CHwb+ESRQZmZWeuqOzp1RPwR+HL6mJmZvSp55uM5Ajgf2DftLyAiYr9iQzMzs1aUZz6eq4CPAkuALcWGY2ZmrS5P4lkfEXcVHomZmQ0JVROPpIPS4mJJ84HbgI092yPipwXHZmZmLajWHc/nytZL51gI4Oi+D8fMzFpdrWkRjgKQtF9EbDNVtCR3LDAzs17J8x7PrRXKbunrQMzMbGio9YznTcABwO6S3lOyaTdgp6IDMzOz1lTrGc9k4ARgFPDOkvIXgQ8WGZSZmbWuWs947gDukHR4RPykiTGZmVkLq/uMx0nHzMz6kqe+NjOzpqqaeCSdlX4e0bxwzMys1dW64zkl/fyvZgRiZmZDQ61ebY9LegIYK2lZSXnP6NRTiw3NzMxaUa1ebSdK+hNgEfCu5oVkZmatrObo1BHxv8CfSxoB7J+KV0bEpsIjMzOzlpRnIri3AdcBT5M1s+0t6eSIuLfg2MzMrAXlmY/nYuCvI2IlgKT9ga8BBxcZmJmZtaY87/EM70k6ABHxc2B4cSGZmVkry3PH0ynpSuCGtP5+oLO4kMzMrJXlSTwfAk4HzkzrPwQuKywiMzNraXnGatsYERdHxHvSZ0FEbKx3HICkGZJWSuqSdG6F7TtKuiltf0DShJJt81L5SknT69UpaWKqoyvVOaLsXH8jKSSVzqRqZmZNVthYbZLagEuB44ApwImSppTtdiqwNiLeCCwALkrHTgHmkM0HNAO4TFJbnTovAhakutamunti2RU4C3igiO9qZmb5FTlI6CFAV0Q8GREvAwuBmWX7zASuTcu3AsdIUipfmO62ngK6Un0V60zHHM3W2VKvBWaVnOdTZInpD339Jc3MrDG5E4+knRusexzwTMn6qlRWcZ+I2AysB0bXOLZa+WhgXapjm3NJOgjYOyK+VStYSadJ6pTUuXr16rzf0czMGlQ38Uj6S0krgJ+l9T+XNCg6F0jagew9pH+ut29EXBER7RHRPmbMmOKDMzMbovLc8SwApgNrACLiEeCtOY7rBvYuWR+fyiruI2kYsHs6T7Vjq5WvAUalOkrLdwX+FPi+pKeBw4AOdzAwM+s/uZraIuKZsqItOQ57CJiUepuNIOss0FG2TwdwclqeDdwTEZHK56RebxOBScCD1epMxyxOdZDqvCMi1kfEXhExISImAPcD74oIv4dkZtZP8rzH84ykvwRC0nCy3mGP1zsoIjZLOoNsdOs24OqIeEzSBUBnRHQAVwHXS+oCXiBLJKT9bgZWAJuB0yNiC0ClOtMpPw4slPQfwNJUt5mZDTDKbhZq7CDtBXweOJZskNC7gbMiYk3x4fWP9vb26Oz0TZGZWSMkLYmIuo8y6t7xRMRvyIbJMTMze9Xy9Gq7VtKokvU9JF1dbFhmZtaq8nQumBoR63pWImItMK24kMzMrJXlSTw7SNqjZ0XSnuTrlGBmZradPAnkc8BPJN1C1rlgNnBhoVGZmVnLytO54DpJS4CjUtF7ImJFsWGZmVmryttk9jOyEZ+HAUjaJyJ+VVhUZmbWsuomHkkfAT4J/JpsxAIBAUwtNjQzM2tFee54zgImt/ILo2Zm1jx5erU9QzZdgZmZ2auW547nSbLRnb8FvDLldURcXFhUZmbWsvIknl+lz4j0MTMz67U83an/HbIZSCPi98WHZGZmrSzPWG2HD9YZSM3MbODJ07ngEno3A6mZmdl2ipyB1MzMbDuFzUBqZmZWSZ47nn8CTgfGAd3AgWndzMysYTXveCS1AX8fEZ6B1MzM+kTNO56I2AK8r0mxmJnZEJDnGc+PJP03cBPwUk9hRPy0sKjMzKxl5Uk8B6afF5SUBXB034djZmatLs/IBUfV28fMzCyvPCMXvE7SVZLuSutTJJ1afGhmZtaK8nSnvgZYBIxN6z8Hzi4qoMHq9qXdHPGZe5h47rc44jP3cPvS7v4OycxsQMqTePaKiJuBPwJExGY8csE2bl/azbzbltO9bgMBdK/bwLzbljv5mJlVkCfxvCRpNFmHAiQdhieG28b8RSvZsGnbXLxh0xbmL1rZTxGZmQ1ceXq1nQN0AG+QdB8wBphdaFSDzLPrNjRUbmY2lFVNPJL+NiJuAdYCbwMmAwJWRsSmJsU3KIwdNZLuCklm7KiR/RCNmdnAVqupbV76+fWI2BwRj0XEo04625s7fTIjh7dtUzZyeBtzp0/up4jMzAauWk1tL0i6G9hPUkf5xoh4V3FhDS6zpo0Dsmc9z67bwNhRI5k7ffIr5WZmtlWtxHM8cBBwPfC55oQzeM2aNs6Jxswsh1pNbVdFxP3AlyPiB+WfPJVLmiFppaQuSedW2L6jpJvS9gckTSjZNi+Vr5Q0vV6dkiamOrpSnSNS+TmSVkhaJul7kvbNE7uZmRWjVuI5WNJY4P2S9pC0Z+mnXsVpSoVLgeOAKcCJkqaU7XYqsDYi3ggsAC5Kx04B5gAHADOAyyS11anzImBBqmttqhtgKdAeEVOBW4HP1ovdzMyKUyvxfBH4HvAmYEnZpzNH3YcAXRHxZES8DCwEZpbtMxO4Ni3fChwjSal8YURsjIingK5UX8U60zFHpzpIdc4CiIjFEfH7VH4/MD5H7GZmVpCqiScivhARbwaujoj9ImJiyWe/HHWPA54pWV+Vyiruk0ZEWA+MrnFstfLRwLpUR7VzQXYXdFelYCWdJqlTUufq1avrfjkzM+udWu/x7BYRvwXOq9S0FhEvFBpZH5N0EtBO9k7SdiLiCuAKgPb29mhiaGZmQ0qtXm03AieQNa0F2cujPQKod9fTDexdsj4+lVXaZ5WkYcDuwJo6x1YqXwOMkjQs3fVscy5JxwLnAW+LiI114jYzswLVamo7If2c2MumtoeASam32QiyzgLl7wN1ACen5dnAPRERqXxO6vU2EZgEPFitznTMYrYO5XMycAeApGnAl4B3RcTzOeI2M7MC1WpqO6jWgfWmvo6IzZLOIJtSoY3sWdFjki4AOiOiA7gKuF5SF/ACWSIh7XczsALYDJweEVtSXNvVmU75cWChpP8g68l2VSqfD7wGuCXrg8Cv/PKrmVn/UXazUGGDtDgt7kT2bOQRsua2qWSJ4/CmRNgP2tvbo7MzT8c9MzPrIWlJRLTX269WU9tRadrr54CDIqI9Ig4GprH9sxozM7Nc8szHMzkilvesRMSjwJuLC8nMzFpZnvl4lkm6Erghrb8fWFZcSGZm1sryJJ5TgA8BZ6X1e4HLC4vIzMxaWt3EExF/IBtHbUHx4ZiZWavL84zHzMyszzjxmJlZU9VNPJJ2qlC2VzHhmJlZq8tzx/OQpMN6ViT9DfDj4kIyM7NWlqdX2/uAqyV9HxhLNgXB0UUGZWZmrStPr7blki4ErgdeBN4aEasKj8zMzFpS3cQj6SrgDWRjtO0P3CnpvyLi0qKDG0xuX9rN/EUreXbdBsaOGsnc6ZOZNa3SXHRmZkNbnqa25cA/pKkHnpJ0KHBxsWENLrcv7WbebcvZsGkLAN3rNjDvtmyUIScfM7Nt1e1cEBGXRMkQ1hGxPiJOLTaswWX+opWvJJ0eGzZtYf6ilf0UkZnZwJWnqW0S8GlgCtkUCQDknAxuSHh23YaGys3MhrI83am/QjY222bgKOA6tg4YasDYUSMbKjczG8ryJJ6REfE9sknjfhkR5wPvKDaswWXu9MmMHN62TdnI4W3MnT65nyIyMxu48nQu2ChpB+CJNO10N9lU0pb0dCBwrzYzs/ryJJ6zgJ2BM4FPkb08enKRQQ1Gs6aNc6IxM8shzwukD6XF35HNzWNmZtZreXq1tQPnAfuW7h8RUwuMa9DxC6RmZvnkaWr7KjCX7EXSPxYbzuDkF0jNzPLL06ttdUR0RMRTqVfbLyPil4VHNoj4BVIzs/zy3PF8UtKVwPeAjT2FEXFbYVENMn6B1MwsvzyJ5xTgTcBwtja1BeDEk4wdNZLuCknGL5CamW0vT+L5i4jwm5A1zJ0+eZtnPOAXSM3MqsnzjOfHkqYUHskgNmvaOD79nj9j3KiRCBg3aiSffs+fuWOBmVkFee54DgMelvQU2TMeAeHu1NvyC6RmZvnkSTwzCo/CzMyGjDwjF7jrtJmZ9Zk8z3jMzMz6TJ6mNsvBQ+aYmeXjxNMHbl/azdxbHmHTH7MZwrvXbWDuLY8AHjLHzKycIqK4yqUZwOeBNuDKiPhM2fYdyWY0PRhYA7w3Ip5O2+YBpwJbgDMjYlGtOiVNBBYCo4ElwN9HxMu1zlFNe3t7dHZ25v6eB/773azbsKnith0E7zt0H9r33bPmHVGtO6Z6d1OD8W5rMMbcn3y9Gudr1pi+uF6SlkREe939iko8ktqAnwNvB1YBDwEnRsSKkn0+DEyNiH+SNAd4d0S8N7039DXgEGAs8F1g/3RYxTol3QzcFhELJX0ReCQiLq92jlqxN5p4Jpz7rbr77MC2I6yOHN72yrs+5YOMlm4Hqm6rd+xA/Us2GGPuT75ejfM1a0xfXa+8iafIzgWHAF0R8WREvEx2NzKzbJ+ZwLVp+VbgGElK5QsjYmNEPAV0pfoq1pmOOTrVQapzVp1zNFX5sN6lg4jWGmS03gCkg3GA0sEYc3/y9Wqcr1ljmn29ikw844BnStZXpbKK+0TEZmA9WVNZtWOrlY8G1qU6ys9V7RzbkHSapE5JnatXr27oi+6x8/CG9u/RM4horUFG6w1AOhgHKB2MMfcnX6/G+Zo1ptnXy92pk4i4IiLaI6J9zJgxDR37yXce0Ktz9gwiWm0w0bGjRtbcVu/YgWowxtyffL0a52vWmGZfryITTzewd8n6+FRWcR9Jw4DdyToAVDu2WvkaYFSqo/xc1c7RZ2ZNG8ek1+5Sc5/yC106iOjc6ZMZObyt4vZa2+odO1ANxpj7k69X43zNGtPs61Vk4nkImCRpoqQRwBygo2yfDuDktDwbuCey3g4dwBxJO6beapOAB6vVmY5ZnOog1XlHnXP0qe+ccyRHvGHP7cp3EJx02D5c/N4Dqw4iWmuQ0XoDkA7GAUoHY8z9ydercb5mjWn29Sq6O/XxwCVkXZ+vjogLJV0AdEZEh6SdgOuBacALwJyIeDIdex7wAWAzcHZE3FWtzlS+H1lngz2BpcBJEbGx1jmqabRXm5mZDYDu1IOZE4+ZWeMGQndqMzOz7TjxmJlZUznxmJlZUznxmJlZU7lzQQWSVgO9nQBvL+A3fRhOX3FcjXFcjRuosTmuxryauPaNiLpv4Dvx9DFJnXl6dTSb42qM42rcQI3NcTWmGXG5qc3MzJrKicfMzJrKiafvXdHfAVThuBrjuBo3UGNzXI0pPC4/4zEzs6byHY+ZmTWVE08fkjRD0kpJXZLObdI5n5a0XNLDkjpT2Z6SviPpifRzj1QuSV9I8S2TdFBJPSen/Z+QdHK189WI42pJz0t6tKSsz+KQdHD6nl3p2FyzyFaJ63xJ3emaPZwGnu3ZNi+dY6Wk6SXlFX+3aaT0B1L5TWnU9Dxx7S1psaQVkh6TdNZAuGY14urXayZpJ0kPSnokxfXvtepSNrL9Tan8AUkTehtvL+O6RtJTJdfrwFTezD/7bZKWSrpzIFyrbUSEP33wIRst+xfAfsAI4BFgShPO+zSwV1nZZ4Fz0/K5wEVp+XjgLkDAYcADqXxP4Mn0c4+0vEeDcbwVOAh4tIg4yKbFOCwdcxdw3KuI63zgYxX2nZJ+bzsCE9Pvs63W7xa4mWzEc4AvAh/KGdfrgYPS8q7Az9P5+/Wa1YirX69Z+g6vScvDgQfSd6tYF/Bh4ItpeQ5wU2/j7WVc1wCzK+zfzD/75wA3AnfWuu7NulalH9/x9J1DgK6IeDIiXiabomFmP8UyE7g2LV8LzCopvy4y95NNnvd6YDrwnYh4ISLWAt8BZjRywoi4l2zaiT6PI23bLSLuj+xvxHUldfUmrmpmAgsjYmNEPAV0kf1eK/5u0/88jwZurfAd68X1XET8NC2/CDxONk17v16zGnFV05Rrlr7379Lq8PSJGnWVXsdbgWPSuRuK91XEVU1Tfo+SxgPvAK5M67Wue1OuVSknnr4zDnimZH0Vtf/C9pUA7pa0RNJpqex1EfFcWv5f4HV1Yiwq9r6KY1xa7sv4zkhNHVcrNWf1Iq7RwLqI2Pxq4kpNG9PI/rc8YK5ZWVzQz9csNR09DDxP9g/zL2rU9cr50/b16dx9/negPK6I6LleF6brtUDSjuVx5Tx/b3+PlwD/Avwxrde67k27Vj2ceAa/t0TEQcBxwOmS3lq6Mf0vqd+7Lg6UOJLLgTcABwLPAZ/rr0AkvQb4Otlkh78t3daf16xCXP1+zSJiS0QcSDa1/SHAm5odQyXlcUn6U2AeWXx/QdZ89vFmxSPpBOD5iFjSrHM2yomn73QDe5esj09lhYqI7vTzeeAbZH8hf51u0Uk/n68TY1Gx91Uc3Wm5T+KLiF+nfyz+CHyZ7Jr1Jq41ZE0lw3oTl6ThZP+4fzUibkvF/X7NKsU1UK5ZimUd2VT3h9eo65Xzp+27p3MX9negJK4ZqckyImIj8BV6f71683s8AniXpKfJmsGOBj7PALpWhT74HkofYBjZA8F5uyAMAAAFR0lEQVSJbH3gdkDB59wF2LVk+cdkz2bms+0D6s+m5Xew7YPNB1P5nsBTZA8190jLe/Yingls+xC/z+Jg+wesx7+KuF5fsvxRsnZsgAPY9mHqk2QPUqv+boFb2PaB7YdzxiSy9vpLysr79ZrViKtfrxkwBhiVlkcCPwROqFYXcDrbPjC/ubfx9jKu15dcz0uAz/TTn/0j2dq5oF+v1TZxNfqPiz81f8nHk/UC+gVwXhPOt1/6pT8CPNZzTrL22e8BTwDfLfkDLODSFN9yoL2krg+QPTzsAk7pRSxfI2uC2UTW5ntqX8YBtAOPpmP+m/Tycy/juj6ddxnQwbb/qJ6XzrGSkt5D1X636XfwYIr3FmDHnHG9hawZbRnwcPoc39/XrEZc/XrNgKnA0nT+R4F/q1UXsFNa70rb9+ttvL2M6550vR4FbmBrz7em/dlPxx7J1sTTr9eq9OORC8zMrKn8jMfMzJrKicfMzJrKicfMzJrKicfMzJrKicfMzJrKiceGHGUjLX8sLV8g6di0/FdphOGHJY2UND+tzy84nrMl7dxHdX1fUntf1NXgec+U9Likr5aVHykpJL2zpOxOSUc2O0YbOIbV38WsdUXEv5Wsvh/4dETcAJDGvtszIrbkqUvSsNg6FlYjziZ71+P3vTh2oPgwcGxErKqwbRXZ+yDfbG5INlD5jseGBEnnSfq5pB8Bk0vKr5E0W9I/AH8HfErSVyV1AK8Blkh6r6Qxkr4u6aH0OSIdf76k6yXdB1yfBoycn/ZZJukf035HpruRWyX9LJ1Dks4ExgKLJS0ui3mGpFtK1o/U1rlVLpfUqZI5YCp859+VLM+WdE1arvZd3qat88cslbRrhTrPkfRo+pydyr5I9nLiXZI+WiGUR4D1kt5e85dkQ0ejb5z6489g+wAHk70lvjOwG9kb2h9L264hzZtC2RwqwO9Klm8kG5AVYB/g8bR8PrAEGJnWTwM+kZZ3BDrJhhY5kmzU3/Fk/+H7SUl9T1M2p1IqHwb8CtglrV8OnJSWe0Y0aAO+D0xN698nvQ1fFv9s4Jo63+WbwBFp+TXAsCrXcZe0/TFgWp3vcCRwJ9m8SD9IZXcCR/b3nwt/+u/jpjYbCv4K+EZE/B4g3c006lhgirZO/rhbGsEZoCMiNqTlvwamSpqd1ncHJgEvk43LtSrF8DDZGHI/qnbCiNgs6X+Ad0q6lWycr39Jm/8uNQUOI5u8bQrZsC2v5rvcB1ycntPcFts3m72F7Dq+lL7DbWTXdmm9E0bEvZKQ9JacMVoLc+Ixy2cH4LCI+ENpYfrH+6XSIuAjEbGobL8jgY0lRVvI9/dvIXAG2WR2nRHxoqSJwMeAv4iItakJbacKx5aOh1W6veJ3AT4j6Vtk43DdJ2l6RPwsR4x5XQh8AujNczBrIX7GY0PBvcCs1FNtV+Cd9Q6o4G7gIz0rkg6sst8i4ENpagEk7S9plzp1v0g2zXQlPyCbuvuDZEkIsubCl8iem7yObC6mSn4t6c2SdgDeXe+7SHpDRCyPiIuAh9h+vpsfkl3HndN3encqyyUi7iYbeXlq3mOsNTnxWMuLbCrnm8gect9F9o9qo84E2lOHgRXAP1XZ70pgBfBTSY8CX6L+nc0VwP+Udy5IsW8heyZyXPpJRDxC1rz1M7LnNfdVqffcdMyPyUborvddzk6dBpaRjeZ9V1ksPyV7DvYg2aykV0ZE3Wa2Mhey7VwuNgR5dGozM2sq3/GYmVlTOfGYmVlTOfGYmVlTOfGYmVlTOfGYmVlTOfGYmVlTOfGYmVlTOfGYmVlT/X+A06XZeXVnTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(N, max_differences, 'o')\n",
    "plt.ylabel(\"max difference of the two gradients\")\n",
    "plt.xlabel(\"different values of N\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows that for very low values of N which makes epsilon large it is not good at all. But the larger values of N which make epsilon smaller gives us a more accurate gradient checking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
