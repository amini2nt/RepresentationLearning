{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import wget\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotThis(x, label ):\n",
    "    xnew = np.linspace(np.arange(x.shape[0]).min(),np.arange(x.shape[0]).max(),300)\n",
    "    spl = make_interp_spline(np.arange(x.shape[0]),x, k=3)\n",
    "    power_smooth = spl(xnew)\n",
    "    xx = plt.plot(xnew,power_smooth, label = label)\n",
    "\n",
    "def vectorized_result(y):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[y] = 1.0\n",
    "    return e\n",
    "\n",
    "def load_mnist():\n",
    "    if not os.path.exists(os.path.join(os.curdir, 'data')):\n",
    "        os.mkdir(os.path.join(os.curdir, 'data'))\n",
    "        wget.download('http://deeplearning.net/data/mnist/mnist.pkl.gz', out='data')\n",
    "\n",
    "    data_file = gzip.open(os.path.join(os.curdir, 'data', 'mnist.pkl.gz'), 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(data_file, encoding='latin1')\n",
    "    data_file.close()\n",
    "\n",
    "    training_inputs = np.squeeze(np.asarray([np.reshape(x, (784, 1)) for x in training_data[0]]))\n",
    "    training_results = np.squeeze(np.asarray([vectorized_result(y) for y in training_data[1]]))\n",
    "\n",
    "    validation_inputs = np.squeeze(np.asarray([np.reshape(x, (784, 1)) for x in validation_data[0]]))\n",
    "    validation_results = np.asarray(validation_data[1])\n",
    "    \n",
    "    test_inputs = np.squeeze(np.asarray([np.reshape(x, (784, 1)) for x in test_data[0]]))\n",
    "    test_results = np.squeeze(np.asarray(test_data[1]))\n",
    "    return training_inputs, training_results, validation_inputs, validation_results, test_inputs, test_results\n",
    "\n",
    "training_inputs, training_results, validation_inputs, validation_results, test_inputs, test_results= load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "def softmaxx(z):\n",
    "        return softmax(z, axis=1)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return expit(z)\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return z*(1-z)\n",
    "\n",
    "def ReLU(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "def ReLU_prime(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n",
    "\n",
    "    \n",
    "class NN(object):\n",
    "    def __init__(self, network_units, activation_fn, lr, batch_size, initialization):\n",
    "        self.network_units = network_units\n",
    "        if activation_fn ==\"ReLU\":\n",
    "            self.activation_fn = ReLU\n",
    "            self.activation_p = ReLU_prime\n",
    "        elif activation_fn == \"sigmoid\":\n",
    "            self.activation_fn = sigmoid\n",
    "            self.activation_p = sigmoid_prime\n",
    "        elif activation_fn == \"tanh\":\n",
    "            self.activation_fn = tanh\n",
    "            self.activation_p = tanh_prime\n",
    "        self.parameters = {}\n",
    "        self.gradients = {}\n",
    "        self.initialization = initialization\n",
    "        self.initialize_weights()\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "    def initialize_weights(self):\n",
    "        \n",
    "        for i in range(len(self.network_units)-1):\n",
    "            \n",
    "            self.parameters[\"b\"+str(i)] = np.zeros((self.network_units[i+1]))\n",
    "            \n",
    "            if self.initialization == \"glorot\":\n",
    "                self.parameters[\"w\"+str(i)] = np.random.uniform( \n",
    "                                -np.sqrt(6./(self.network_units[i]+self.network_units[i+1])), \n",
    "                                np.sqrt(6./(self.network_units[i]+self.network_units[i+1])),\n",
    "                                size=(self.network_units[i],self.network_units[i+1])\n",
    "                                                              )\n",
    "            elif self.initialization == \"normal\":\n",
    "                self.parameters[\"w\"+str(i)] = np.random.normal(scale=1, size=(self.network_units[i],self.network_units[i+1]))\n",
    "            \n",
    "            elif self.initialization == \"zero\":\n",
    "                self.parameters[\"w\"+str(i)] = np.zeros(shape=(self.network_units[i],self.network_units[i+1]))\n",
    "    \n",
    "            self.gradients[\"b\"+str(i)] = np.zeros((self.network_units[i+1]))\n",
    "            self.gradients[\"w\"+str(i)] = np.zeros((self.network_units[i], self.network_units[i+1]))\n",
    "\n",
    "            \n",
    "    def num_parameters(self):\n",
    "        pars = 0.0\n",
    "        for i in range(len(self.network_units)-1):\n",
    "            pars += self.network_units[i]\n",
    "            pars += (self.network_units[i] * self.network_units[i+1])\n",
    "        print(\"we have \", pars/1000000, \" parameters\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.h1 = self.activation_fn(np.dot(x, self.parameters[\"w0\"]) + self.parameters[\"b0\"])\n",
    "        self.h2 = self.activation_fn(np.dot(self.h1, self.parameters[\"w1\"]) + self.parameters[\"b1\"])\n",
    "        self.out = expit(np.dot(self.h2, self.parameters[\"w2\"]) + self.parameters[\"b2\"])\n",
    "        return self.out\n",
    "    \n",
    "    def loss(self, y):\n",
    "        self.y = y\n",
    "        l = -y*np.log(self.out+1e-8)-(1-y)*np.log(1-self.out+1e-8)\n",
    "        m_b_l = np.sum(l, axis=1)\n",
    "        self.lossy = np.mean(m_b_l)\n",
    "        return self.lossy\n",
    "    \n",
    "    def backward(self):\n",
    "        \n",
    "        \n",
    "        dout = (-self.y/(self.out+1e-8)) + (1-self.y) * (1/(1-self.out+1e-8))\n",
    "        del2 = self.out*(1-self.out)*dout\n",
    "        doutdw2 = np.dot(self.h2.T, del2)/len(self.x)\n",
    "        doutdb2 = np.mean(del2, axis=0)\n",
    "        \n",
    "        \n",
    "        douth2 = np.dot(del2, self.parameters[\"w2\"].T)\n",
    "        del1 = self.activation_p(self.h2)*douth2\n",
    "        doutdw1 = np.dot(self.h1.T, del1)/len(self.x)\n",
    "        doutdb1 = np.mean(del1, axis=0)\n",
    "        \n",
    "        douth1 = np.dot(del1, self.parameters[\"w1\"].T)\n",
    "        del0 = self.activation_p(self.h1)*douth1\n",
    "        doutdw0 = np.dot(self.x.T, del0)/len(self.x)\n",
    "        doutdb0 = np.mean(del0, axis=0)\n",
    "        \n",
    "        self.gradients[\"w2\"] = doutdw2\n",
    "        self.gradients[\"b2\"] = doutdb2\n",
    "        self.gradients[\"w1\"] = doutdw1\n",
    "        self.gradients[\"b1\"] = doutdb1\n",
    "        self.gradients[\"w0\"] = doutdw0\n",
    "        self.gradients[\"b0\"] = doutdb0\n",
    "        \n",
    "    def grad_check(self, single_training_example, single_training_result):\n",
    "        output = self.forward(single_training_example)\n",
    "        loss = self.loss(single_training_result)\n",
    "        self.backward()\n",
    "        epsilon = 1e-4\n",
    "        \n",
    "        #finite_difference:\n",
    "        numerical_gradient = 0.0\n",
    "        max_difference = 0\n",
    "        for i in range(10):\n",
    "            #do left\n",
    "            self.parameters[\"w0\"][0][i] = self.parameters[\"w0\"][0][i] + epsilon\n",
    "            self.forward(single_training_example)\n",
    "            left_loss = self.loss(single_training_result)\n",
    "            self.parameters[\"w0\"][0][i] = self.parameters[\"w0\"][0][i] - (2 * epsilon)\n",
    "            self.forward(single_training_example)\n",
    "            right_loss = self.loss(single_training_result)\n",
    "            self.parameters[\"w0\"][0][i] = self.parameters[\"w0\"][0][i] + epsilon\n",
    "\n",
    "            numerical_gradient = (left_loss - right_loss)/ (2*epsilon)\n",
    "            print(\"numerical_gradient\", numerical_gradient)\n",
    "            \n",
    "            analytic_gradient = self.gradients[\"w0\"][0][i] \n",
    "            print(\"analytic gradient\", analytic_gradient)\n",
    "            current_difference = np.abs(numerical_gradient - analytic_gradient)\n",
    "            print(\"difference: \",current_difference)\n",
    "            if current_difference > max_difference:\n",
    "                max_difference = current_difference\n",
    "        #only once we are done with with this for loop for our N then we append Max_difference to our list of Max differences\n",
    "        \n",
    "        \n",
    "                \n",
    "\n",
    "    def update(self):\n",
    "        for ind in [\"w0\", \"w1\", \"w2\", \"b0\", \"b1\", \"b2\"]:\n",
    "            self.parameters[ind] -= self.lr * self.gradients[ind]\n",
    "\n",
    "    def train(self,training_inputs, training_results, epochs, validation_inputs, validation_targets):\n",
    "        loss = np.zeros((epochs))\n",
    "        accuracy = np.zeros((epochs))\n",
    "        for epoch in range(epochs):\n",
    "            shuffle(training_inputs, training_results, random_state=0)\n",
    "            mini_batches = [training_inputs[k:k+self.batch_size] for k in range(0, training_inputs.shape[0], self.batch_size)]\n",
    "            mini_batches_results = [training_results[k:k+self.batch_size] for k in range(0, training_results.shape[0], self.batch_size)]\n",
    "            #print(mini_batches_results[0])\n",
    "            #break\n",
    "            current_loss=0\n",
    "            for mini_batch_index in range(len(mini_batches)):\n",
    "                self.forward(mini_batches[mini_batch_index])\n",
    "                current_loss += self.loss(mini_batches_results[mini_batch_index])\n",
    "                self.backward()\n",
    "                self.update()\n",
    "            current_loss /= len(mini_batches)\n",
    "            loss[epoch] = current_loss\n",
    "            print(\"loss \", current_loss)\n",
    "            currentAccuray = self.test(validation_inputs, validation_targets)\n",
    "            accuracy[epoch] = currentAccuray\n",
    "        self.grad_check(np.asarray([training_inputs[0]]), training_results[0])\n",
    "        return loss, accuracy \n",
    "    def test(self, validation_inputs, validation_targets):\n",
    "        results = self.forward(validation_inputs)\n",
    "        labels = np.argmax(results, axis=1)\n",
    "        accuracy = np.sum(labels == validation_targets)/100\n",
    "        print(\"validation accuracy\", accuracy)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss  1.169024070649404\n",
      "validation accuracy 91.51\n",
      "numerical_gradient 0.0\n",
      "analytic gradient 0.0\n",
      "difference:  0.0\n",
      "numerical_gradient 0.0\n",
      "analytic gradient 0.0\n",
      "difference:  0.0\n",
      "numerical_gradient 0.0\n",
      "analytic gradient 0.0\n",
      "difference:  0.0\n",
      "numerical_gradient 0.0\n",
      "analytic gradient 0.0\n",
      "difference:  0.0\n",
      "numerical_gradient 0.0\n",
      "analytic gradient 0.0\n",
      "difference:  0.0\n",
      "numerical_gradient 0.0\n",
      "analytic gradient 0.0\n",
      "difference:  0.0\n",
      "numerical_gradient 0.0\n",
      "analytic gradient 0.0\n",
      "difference:  0.0\n",
      "numerical_gradient 0.0\n",
      "analytic gradient 0.0\n",
      "difference:  0.0\n",
      "numerical_gradient 0.0\n",
      "analytic gradient 0.0\n",
      "difference:  0.0\n",
      "numerical_gradient 0.0\n",
      "analytic gradient 0.0\n",
      "difference:  0.0\n"
     ]
    }
   ],
   "source": [
    "myNet = NN([784,500,500,10], \"ReLU\", lr=0.01, batch_size=32, initialization=\"glorot\")\n",
    "glorot_relu_loss, glorot_relu_accuracy = myNet.train(training_inputs, training_results, 1, validation_inputs, validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.03780831117196146\n",
      "-0.03770831117196146\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNet = NN([784,500,500,10], \"ReLU\", lr=0.01, batch_size=32, initialization=\"normal\")\n",
    "normal_relu_loss, normal_relu_accuracy = myNet.train(training_inputs, training_results, 10, validation_inputs, validation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNet = NN([784,500,500,10], \"ReLU\", lr=0.01, batch_size=32, initialization=\"zero\")\n",
    "zero_relu_loss, zero_relu_accuracy = myNet.train(training_inputs, training_results, 10, validation_inputs, validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plotThis(glorot_relu_loss, \"glorot relu loss\")\n",
    "plotThis(zero_relu_loss, \"zero relu loss\")\n",
    "plotThis(normal_relu_loss, \"normal relu loss\")\n",
    "\n",
    "plt.title(\"training loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plotThis(glorot_relu_accuracy, \"glorot relu accuracy\")\n",
    "plotThis(zero_relu_accuracy, \"zero relu accuracy\")\n",
    "plotThis(normal_relu_accuracy, \"normal relu accuracy\")\n",
    "\n",
    "plt.title(\"evaluation accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNet = NN([784,500,500,10], \"tanh\", lr=0.01, batch_size=32, initialization=\"glorot\")\n",
    "glorot_tanh_loss, glorot_tanh_accuracy = myNet.train(training_inputs, training_results, 10, validation_inputs, validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNet = NN([784,500,500,10], \"tanh\", lr=0.01, batch_size=32, initialization=\"normal\")\n",
    "normal_tanh_loss, normal_tanh_accuracy = myNet.train(training_inputs, training_results, 10, validation_inputs, validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNet = NN([784,500,500,10], \"tanh\", lr=0.01, batch_size=32, initialization=\"zero\")\n",
    "zero_tanh_loss, zero_tanh_accuracy = myNet.train(training_inputs, training_results, 10, validation_inputs, validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plotThis(glorot_tanh_loss, \"glorot tanh loss\")\n",
    "plotThis(zero_tanh_loss, \"zero tanh loss\")\n",
    "plotThis(normal_tanh_loss, \"normal tanh loss\")\n",
    "\n",
    "plt.title(\"training loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plotThis(glorot_tanh_accuracy, \"glorot tanh accuracy\")\n",
    "plotThis(zero_tanh_accuracy, \"zero tanh accuracy\")\n",
    "plotThis(normal_tanh_accuracy, \"normal tanh accuracy\")\n",
    "\n",
    "plt.title(\"evaluation accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNet = NN([784,500,500,10], \"sigmoid\", lr=0.01, batch_size=32, initialization=\"glorot\")\n",
    "glorot_sigmoid_loss, glorot_sigmoid_accuracy = myNet.train(training_inputs, training_results, 10, validation_inputs, validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNet = NN([784,500,500,10], \"sigmoid\", lr=0.01, batch_size=32, initialization=\"normal\")\n",
    "normal_sigmoid_loss, normal_sigmoid_accuracy = myNet.train(training_inputs, training_results, 10, validation_inputs, validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNet = NN([784,500,500,10], \"sigmoid\", lr=0.01, batch_size=32, initialization=\"zero\")\n",
    "zero_sigmoid_loss, zero_sigmoid_accuracy = myNet.train(training_inputs, training_results, 10, validation_inputs, validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plotThis(glorot_sigmoid_loss, \"glorot sigmoid loss\")\n",
    "plotThis(zero_sigmoid_loss, \"zero sigmoid loss\")\n",
    "plotThis(normal_sigmoid_loss, \"normal sigmoid loss\")\n",
    "\n",
    "plt.title(\"training loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plotThis(glorot_sigmoid_accuracy, \"glorot sigmoid accuracy\")\n",
    "plotThis(zero_sigmoid_accuracy, \"zero sigmoid accuracy\")\n",
    "plotThis(normal_sigmoid_accuracy, \"normal sigmoid accuracy\")\n",
    "\n",
    "plt.title(\"evaluation accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
