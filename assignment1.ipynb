{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "import os\n",
    "import gzip\n",
    "import cPickle\n",
    "import wget\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        self.num_layers = len(layers)\n",
    "        self.layers = layers\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        self.biases = [np.zeros((y, 1)) for y in self.layers[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.layers[:-1], self.layers[1:])]\n",
    "    \n",
    "    def forward(self,x):\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []    \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = softmax(z)\n",
    "            activations.append(activation)\n",
    "        return activations, zs\n",
    "        \n",
    "        \n",
    "    def update(self,mini_batch, eta, n ):\n",
    "        nabla_b = [np.zeroes(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            activations, zs = self.forward(x)\n",
    "            delta_nabla_b, delta_nabla_w = self.backward(x, y, activations, zs)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "    \n",
    "    def backward(self, x, y, zs, activations):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        difference = activations[-1] - y\n",
    "        nabla_b[-1] = difference\n",
    "        nabla_w[-1] = np.dot(difference, activations[-2].transpose())\n",
    "        for l in xrange(2, self.num_layers -2, 0, -1):\n",
    "            dell = np.dot(self.weights[l+1].transpose(), difference) * ( softmax(zs[1])*(1-softmax(zs[1])) ) \n",
    "            nabla_b[-l] = difference\n",
    "            nabla_w[-l] = np.dot(difference, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "        \n",
    "    def softmax(z):\n",
    "        e = np.exp(float(z))\n",
    "        return (e/np.sum(e))\n",
    "\n",
    "    def loss(a, y):\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    def train(self,training_data, mini_batch_size, epochs, eta):\n",
    "        n = len(training_data)\n",
    "        for epoch in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0, n, minibatches)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update( mini_batch, eta, len(training_data))\n",
    "\n",
    "        \n",
    "        \n",
    "    #def test(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_result(y):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[y] = 1.0\n",
    "    return e\n",
    "\n",
    "def load_mnist():\n",
    "    if not os.path.exists(os.path.join(os.curdir, 'data')):\n",
    "        os.mkdir(os.path.join(os.curdir, 'data'))\n",
    "        wget.download('http://deeplearning.net/data/mnist/mnist.pkl.gz', out='data')\n",
    "\n",
    "    data_file = gzip.open(os.path.join(os.curdir, 'data', 'mnist.pkl.gz'), 'rb')\n",
    "    training_data, validation_data, test_data = cPickle.load(data_file)\n",
    "    data_file.close()\n",
    "\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in training_data[0]]\n",
    "    training_results = [vectorized_result(y) for y in training_data[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in validation_data[0]]\n",
    "    validation_results = validation_data[1]\n",
    "    validation_data = zip(validation_inputs, validation_results)\n",
    "\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in test_data[0]]\n",
    "    test_data = zip(test_inputs, test_data[1])\n",
    "    return training_data, validation_data, test_data\n",
    "train_data, valid_data, test_data = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = NN([784,40,40,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.train(train_data, 32, 10, 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
